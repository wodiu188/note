# 虫术

pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple/将pip源改为清华大学的镜像

## 开启scrapy项目之旅

- pip install scrapy#安装scrapy框架

- 可以用codecs.open()来替代原生open

- 用UnicodeDammit("字符串",[编码类型(列表)])
- 创建一个项目
- scrapy startproject [项目名]


- 创建一个scrapy


- scrapy genspider 项目名 网站名如 scrapy genspider example example.com

- 抓取一个scrapy项目

- scrapy crawl 项目名

- ##### 创建Item

- 管理字典一样管理






#### 间接爬虫

- 导入一个请求链接的包

- from scrapy.http import Request

- 然后用Request(url = '',callback=[函数名])来返回


```python
    def parse(self, response):
        rss_page = BeautifulSoup(response.body,"html.parser")
        rss_link = set([item['href'] for item in rss_page.find_all('a')])
        for link in rss_link:
            yield Request(url=link,callback=self.parse_feed)
```

- 然后在parse_feed里面继续处理请求的链接



### 管道

- 经过parse函数的爬取返回后就到了管道,进行对数据进行处理

- 通过查看配置文件


```python
#用来设定开启的管道接口采取完整的名称,:后面的是优先级,数值越低优先级越高还可以在pipelines里面添加其他的管道
ITEM_PIPELINES = {
   'chinanews_cawler.pipelines.ChinanewsCawlerPipeline': 300,
}
```





## Scrapy的运行和配置

- 对scrapy的设定优先级


```
最高:命令行
其次:模块设定例如myproject.settings
第三:命令模块设定全局默认设定存储在scrapy.settings.default_settings模块中
最低:全局默认设定
```

##### 测试案例

```python
#####
#__init__.py文件用来写spider
#####


# This package will contain the spiders of your Scrapy project
#
# Please refer to the documentation for information on how to create and manage
# your spiders.
from scrapy.spiders import Spider
from scrapy.http import Request
from ..items import myItem
from bs4 import BeautifulSoup

class ChinaNewsSpider(Spider):
    name = "chinanews"
    allowed_domains = ['chinanews.com']
    start_urls = ('http://www.chinanews.com/rss/rss_2.html',)

    def parse(self, response):
        rss_page = BeautifulSoup(response.body,"html.parser")
        rss_link = set([item['href'] for item in rss_page.find_all('a')])
        for link in rss_link:
            yield Request(url=link,callback=self.parse_feed)

    def parse_feed(self,response):
        rss = BeautifulSoup(response.body,'lxml')
        print(rss)
        for item in rss.find_all('item'):
            feedField = myItem()
            feedField['title'] = item.title.text
            feedField['link'] = item.link.text
            feedField['desc'] = item.description.text
            feedField['pub_date'] = item.pubdate.text
            yield feedField
```

```python
#######
#items.py用来写传递数据的items类
#######

# -*- coding: utf-8 -*-

# Define here the models for your scraped items
#
# See documentation in:
# https://doc.scrapy.org/en/latest/topics/items.html

import scrapy
from scrapy.item import Item,Field

class myItem(Item):
    title = Field()#标题
    link = Field()#新闻详情
    desc = Field()#新闻综述
    pub_date = Field()#发布日期


class ChinanewsCawlerItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    pass

```

#### 对于Scrapy工程管理的命令

- 对于开发与测试环境都要基于Scrapy

- 开发环境中以scrapy-client作为自动化部署工具

- 生产环境中将以Scrapyd工具为宿主




##### Scrapyd可以采用RESTful API来对服务进行检查

```java
//默认服务地址为http://localhost:6800
一、采用GET请求
    1.查询API:
        http://localhost:6800/daemonstatus.json
    结果:
        {"node_name": "DESKTOP-8D37QB2", "status": "ok", "pending": 0, "running": 0, "finished": 0}

    2.获取已上传的列表
        http://localhost:6800/listproject.json

	3.查询可用版本列表
        http://localhost:6800/listversions.json?project = 项目名

	4.获取当前版本可用的spider
        http://localhost:6800/listspiders.json?project = 项目名[&_version=版本号]
        
    5.获取项目中待定的、正在运行的、已完成的列表
        http://localhost:6800/listjobs.json?project = 项目名

二、采用POST请求
   	1.删除项目
    	http://localhost:6800/delproject.json -d project = 项目名	
	
	2.添加一个项目版本
        http://localhost:6800/addversion.json -F project=项目名 -F version=r23 -F egg=@myproject.egg

    3.加载运行指定的蜘蛛
		http://localhost:6800/schedule.json -d project=项目名 -d spider=蜘蛛名 -d [setting=声明的设置项目] [-d jobid=工作id用来取代默认的id] [-d _version=版本号]

    4.中制蜘蛛的作业
        http://localhost:6800/cancel.json -d project=项目名 -d job=作业编号

    5.删除已经上传的版本
		http://localhost:6800/delversion.json -d project=项目名 -d version=r23
```



##### 使用scrapyd-client

```
一、展示项目或者爬虫
    scrapyd-client project
    列出服务器上所有的项目

    scrapyd-client -t http://scrapyd.example.net project
    列出指定服务器地址上的项目
	
	scrapyd-client spiders
	列出所有的spider
	
	scrapyd-client spiders -p sina
	列出指定项目(sina)下的spider
	
二、启动一个或者多个爬网任务
	scrapyd-client schedule
	启动任意爬虫
	
	scrapyd-client schedule -p sina *	
	启动指定项目(sina)中的所有爬虫

	scrapyd-client schedule -p *	
	支持通配符,启动所有以_daily结尾的项目中的蜘蛛


```

## scrapyd-deploy

```cmd
	#命令使用
  -h, --help            show this help message and exit
  -p PROJECT, --project=PROJECT
                        the project name in the target
  -v VERSION, --version=VERSION
                        the version to deploy. Defaults to current timestamp
  -l, --list-targets    list available targets
  -a, --deploy-all-targets
                        deploy all targets
  -d, --debug           debug mode (do not remove build dir)
  -L TARGET, --list-projects=TARGET
                        list available projects on TARGET
  --egg=FILE            use the given egg, instead of building it
  --build-egg=FILE      only build the egg, don't deploy it


```



- 使用scrapy-deploy 来提交一个项目

- 提交格式scrapyd-deploy 服务器名字 -p 项目名称

- 在提交之前需要修改scrapy项目的scrapy.cfg配置文件


```cfg

[deploy:demo(服务器名字)]
url = http://localhost:6800/
project = chinanews_cawler(爬虫项目名)
目前为止我没血密码
#username = 
#password = 
#version = HG或者Git
```

- 使用scrapyd-deploy -l可以查询全部部署目标

- 使用scrapyd-deploy -l example 可以查看具体的部署目标



## 深入了解spider

- 查看spider源码发现,初始化函数直接检查name和start_urls是否存在不存在就报错
- 初始化spider后并不会调用爬虫而是使用start_requests才开始爬虫,在scrapy2.3.0中通过method_is_overridden来检查子类中是否重写了make_requests_from_url方法如果重写就提示这个函数即将废用如果没有重写就循环调用Request()来创建出多个请求
- 如果想要自己实现start_requests,(比如想要一开始先登录在执行爬取就可以将该方法修改位POST请求)通过重写的方式来进行修改这里第三个方法给予一个实例

```python
    def __init__(self, name=None, **kwargs):
        if name is not None:
            self.name = name
        elif not getattr(self, 'name', None):
            raise ValueError("%s must have a name" % type(self).__name__)
        self.__dict__.update(kwargs)
        if not hasattr(self, 'start_urls'):
            self.start_urls = []
    def start_requests(self):
        cls = self.__class__
        if method_is_overridden(cls, Spider, 'make_requests_from_url'):
            warnings.warn(
                "Spider.make_requests_from_url method is deprecated; it "
                "won't be called in future Scrapy releases. Please "
                "override Spider.start_requests method instead (see %s.%s)." % (
                    cls.__module__, cls.__name__
                ),
            )
            for url in self.start_urls:
                yield self.make_requests_from_url(url)
        else:
            for url in self.start_urls:
                yield Request(url, dont_filter=True)
	#重写函数实现用POST方法来请求
	def start_requests(self):
        return [scrapy.FormRequest('http:www.example.com/login',formata={'user':name,'pass':secret},callback=self.logged_in)]
```

----------------------------------------------------------------------

### 包scrapy.spider.Spider

- ​	name(唯一,重要属性用来定位spider)
- ​	allowed_domains(可选,用来指示爬取域名范围)
- ​	start_urls(没有特定指定的URL时默认采取的爬取地址)
- ​	start_requests()[返回一个可迭代对象]
- ​	parse(response)
- ​	closed(reason)



------------------------------------------

### 通用spider

​	特点:

- 爬取大量网站
- 不会将整个网站都爬取完毕
- 逻辑非常简单
- 并行爬取大量网站以避免被某个网站限制爬取速度

Scrapy提供了4个通用爬虫他们都是

```python
#XMLFeedSpider--用于爬取符合XML文档的基类;


	scrapy genspider -t xmlfeed myxmlspider sina.com.cn
	
    
1.必须继承iterator(枚举)属性,指明时针对文档的根节点还是针对指定节点进行搜索
2.不需要重写parse方法,而是重写parse_node.XMLFeedSpider按照itertag属性中指定的标签筛选出节点的集合,然后之一调用parse_node方法并将该节点作为处理参数传入
"""
可重写的参数有个
iterator--用于声明枚举类型.可选值为iternodes、xml、html
	iternodes:使用的是正则来迭代器
	xml:使用select的选择器,该迭代器采用了DOM来进行分析,在运行是需要将所有的DOM加载完毕才能执行对于大数据量是
	html:和xml一样
itertag--用于指定筛选那些xml标签，默认为item
	在爬取的xml文档中从itertag指定的节点中开始
namespaces--具有特殊命名空间的xml文档可以通过此元组属性指定。
"""
XMLFeedSpider内部集成了XPath会针对itertag进行访问并将结果返回到parse_node
由于HTML5的大量应用而且html5也严格遵从xml,因此用XMLFeedSpider来爬取是很好的选择


#CSVFeedSpider--用于爬取CSV文件的蜘蛛
	采用三个属性来调节
    delimiter 设置每行字段值间的分割符设置为None采用","作为分隔符
    quotechar设置采用的引号
    headers在CSV文件中包含的用来提取字段的名称列表
    在parse_row(response,row):
        可以采取row['字符值']来获取返回的CSV中的值
#CrawlSpider--用于进行间接递进爬取的蜘蛛
	rules = {爬取规则}内部采用Rule()构造对象
    Rule(self, link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=identity)
    link_extractor链接提取器(现在一般都是LinkExtractor)
    callback回调函数
    cb_kwargs=给回调函数的字典
    follow是否从response中提取的链接进行跟进
    process_links从链接提取器中获取列表时会调用
    process_request该规则提取每个request会调用该函数
    
#SitemapSpider--从Sitemap.xml文件跟随进入网站进行深度爬网的蜘蛛
	基本没用因为现在要爬的网站几乎都不提供
	stiemap_urls包含要爬取的URL的Sitemap的URL列表(list),也可以指定为一个robots.txt,Spider会从中分析URL
    sitemap_rules正则
    sitemap_follow跟进正则列表
    siteamp_alternate_links可选的链接(比如一个网站有英语版)
    	例如:
            <url>
            	<loc>http://example.com</loc>
                <xhtml:link rel="alternate" hreflang="de" href="http://example.com/de/">
            </url>
		对于这样的链接如果关闭siteamp_alternate_links就不获取
```





看网上:

https://www.cnblogs.com/xieqiankun/p/know_middleware_of_scrapy_1.html

这个网址介绍了如何使用中间件如何修改代理ip,UA,Cookie

测试网址IP代理http://exercise.kingname.info/exercise_middleware_ip

测试UAhttp://exercise.kingname.info/exercise_middleware_ua。

测试Cookiehttp://exercise.kingname.info/exercise_login_success

1.定义一:更换代理IP，更换Cookies，更换User-Agent，自动重试。

```python
#1.process_spider_input当response经过中间件时调用处理response,
#返回异常(不会调用其他中间件的process_spider_input)或者None(继续处理)
    def process_spider_input(self, response, spider):
        # Called for each response that goes through the spider
        # middleware and into the spider.
        # Should return None or raise an exception.
        return None
#2.当process_spider_input抛出异常时调用
#当返回None则Scrapy继续处理异常,当所有的中间件的process_spider_exception都经过到达引擎异常将被忽视和记录
#返回一个可迭代对象,中间链的process_spider_output其他的process_spider_exception
    def process_spider_exception(self, response, exception, spider):
        # Called when a spider or process_spider_input() method
        # (from other spider middleware) raises an exception.

        # Should return either None or an iterable of Response, dict
        # or Item objects.
        pass

```

2.scrapy内置中间件参考:

​	DepthMiddleware用于跟踪request在被爬取的网站中的深度的中间件,用来限制深度

​		DEPTH_LIMIT允许的最大深度

​		DEPTH＿STATS获取爬取状态

​		DEPTH＿PRIORITY根据深度获取request状态

​	HttpErrorMiddleware决定处理其他的错误状态码200~300是成功的

​		可以通过handle_httpstatus_list属性或HTTPERROR_ALLOWED_CODES来指定处理那些Spider的response返回值

​		也可以用Request.meta['handle_httpstatus_list']

​	OffsiteMiddleware过滤所有主机名不在Spider属性allowed_domains属性，即使该request的网站不在允许列表里，offsite中间件也会允许该request

​	RefererMiddleware根据request的response的URL来设置request_referer

​	UrlLengthMiddleware过滤出URL长度比URLLENGTH_LIMIT设置的request对象

## 测试

​	@url url

​	@returns item(s)|request(s) [min [max]] 设置返回spider的items和requests的上界和下界

​	@scrapes field_1 field_2 ...  检查字段中是否存在

​	自定义Contracts

-------------------------------------------------------------------------------------------------------------------------------------------------------

​	需要开启SPIDER_CONTRACTS{

​		"路径":优先级,	

​	}

​	需要覆盖Contract(method,*args*)

​		method关联的回调函数

​		args----docstring的argument列表

​	需要覆盖

​		adjust_request_args(args)一个字典包含了所有request对象参数的默认值

​		pre_process(response):-----该函数在sample request接收到response后,传送给回调函数前被调用

​		post_process(output):-------该函数处理回调函数的输出迭代器在传输的时候会被序列化

​	scrapy check -l用来测试contract



## 运行时调试

​	使用scrapy parse --spider==项目名 -c 自定义的Item -d 深度 <item_url>

​	--verbose或-v查看各层的状态

​	检查单个start_url

​	在浏览器中打开当前爬取的结果



## 用shell来调试

​	scrapy shell <url>

- shelp()打印可用对象以及快捷命令
- fetch(request_or_url) 根据参数来获取response并更新相关类
- view(response)在本机的浏览器打开给定response,会在response的body中添加一个<base>使得外部链接能正确显示。

可用的scrapy对象

​	crawler当前的crawler对象

​	spider处理url的spider

​	request最近获取到的页面的request对象。可以使用replace(),或者使用fetch快捷方式获取request

​	sel获取最近的response构建的Selector

​	settings返回一个settings

​	inspect_response(response,self)加入断点

```python
#开启
scrapy shell www.baidu.com
#抓取
fetch(<url>)
#提取数据
response.body#爬取的网页
```



## 使用telnet和Guppy来进行内存调试

- ### 	telnet

  官方文档https://doc.scrapy.org/en/latest/topics/telnetconsole.html

  首先需要在开启爬虫的情况下,来运行

```
TELNETCONSOLE_USERNAME = "YH"
TELNETCONSOLE_PASSWORD = "123456"
在settings.py中可以设置用户名和密码

在cmd中使用telnet localhost 6023来链接
	est()快速查看状态
	engine.pause()暂停
	engine.unpause()恢复
	engine.stop()停止
	
	Telnet 终端信号
	scrapy.telnet.update_telnet_vars(telnet_vars)
	prefs()用来查看每个的活动状态


	用来查看当前最老的HtmlResponse是哪一个url导致的
    >>> from scrapy.utils.trackref import get_oldest
    >>> r = get_oldest('HtmlResponse')
    >>> r.url
    'http://www.somenastyspider.com/product.php?pid=123'
    
    遍历所有的HtmlResponse
    >>> from scrapy.utils.trackref import iter_all
    >>> [r.url for r in iter_all('HtmlResponse')]
    ['http://www.somenastyspider.com/product.php?pid=123',
     'http://www.somenastyspider.com/product.php?pid=584',
    ...]
    
```

- ### Guppy

  可以使用setuptools的easy_install来安装

  对于Telnet终端提供了快捷方式

  ```cmd
  #需要用pip install guppy来安装
  >>> x = hpy.heap()
  >>> x.bytype
  Partition of a set of 297033 objects. Total size = 52587824 bytes.
   Index  Count   %     Size   % Cumulative  % Type
       0  22307   8 16423880  31  16423880  31 dict
       1 122285  41 12441544  24  28865424  55 str
       2  68346  23  5966696  11  34832120  66 tuple
       3    227   0  5836528  11  40668648  77 unicode
       4   2461   1  2222272   4  42890920  82 type
       5  16870   6  2024400   4  44915320  85 function
       6  13949   5  1673880   3  46589200  89 types.CodeType
       7  13422   5  1653104   3  48242304  92 list
       8   3735   1  1173680   2  49415984  94 _sre.SRE_Pattern
       9   1209   0   456936   1  49872920  95 scrapy.http.headers.Headers
  <1676 more rows. Type e.g. '_.more' to view.>
  ```

- ### muppy

  ```
  pip install Pympler
  下面是一个使用muppy查看堆中所有可用python对象的示例：
  
  >>> from pympler import muppy
  >>> all_objects = muppy.get_objects()
  >>> len(all_objects)
  28667
  >>> from pympler import summary
  >>> suml = summary.summarize(all_objects)
  >>> summary.print_(suml)
                                 types |   # objects |   total size
  ==================================== | =========== | ============
                           <class 'str |        9822 |      1.10 MB
                          <class 'dict |        1658 |    856.62 KB
                          <class 'type |         436 |    443.60 KB
                          <class 'code |        2974 |    419.56 KB
            <class '_io.BufferedWriter |           2 |    256.34 KB
                           <class 'set |         420 |    159.88 KB
            <class '_io.BufferedReader |           1 |    128.17 KB
            <class 'wrapper_descriptor |        1130 |     88.28 KB
                         <class 'tuple |        1304 |     86.57 KB
                       <class 'weakref |        1013 |     79.14 KB
    <class 'builtin_function_or_method |         958 |     67.36 KB
             <class 'method_descriptor |         865 |     60.82 KB
                   <class 'abc.ABCMeta |          62 |     59.96 KB
                          <class 'list |         446 |     58.52 KB
                           <class 'int |        1425 |     43.20 KB
  ```





## HTTP的解析

1. 请求方法
   - Get 是一种只读行为
   - Post 在URL上发布指定新信息。并且服务器确保数据已经被存储一次
   - Put 类似Post可能触发多次存储过程类似一种“更新”行为
   - DELETE 删除给定为止的信息
   - HEAD 服务器获取信息，但是只关心消息头，服务器会向Get一样来处理它，Get延申
   - Options 给客户端提供一个便捷的途径来弄清这个URL支持那些HTTP方法，Get延申

|----------------------

以上的方法对应requests包

比如：

```
import requests
if __name__ == "__main__":
    r = requests.get("http://httpbin.org/html")
    print(r.text)
    r = requests.post("http://httpbin.org/html")
    print(r.text)
    r = requests.delete("http://httpbin.org/html")
    print(r.text)
    r = requests.head("http://httpbin.org/html")
    print(r.text)
    r = requests.options("http://httpbin.org/html")
    print(r.text)
```

|----------------------



参数化页面

​	带有查询字符串

​	http：//。。。。。。？参数

​	路由

​	http：//。。。。。。/asdf-asd-zzzz-asdfa.html

​	提交表单

​	http请求头解析:https://www.cnblogs.com/wanghuaqiang/p/12093563.html

##  Request对象

​	构造参数

```python
def __init__(self, url, callback=None, method='GET', headers=None, body=None,
             cookies=None, meta=None, encoding='utf-8', priority=0,
             dont_filter=False, errback=None, flags=None):
```

url:要请求的url

| 参数        | 作用                                                         |
| :---------- | ------------------------------------------------------------ |
| callback    | 请求成功后调用指定的参数并返回response,如果没有指定默认反调用parse函数 |
| method      | 请求方式例如:"GET"                                           |
| body        |                                                              |
| headers     | 请求头                                                       |
| cookies     | 传递一个字典或者传递一个字典列表                             |
| meta        | 可以存储数据或者对request功能的扩展                          |
| encoding    | 编码格式                                                     |
| priority    | 优先级                                                       |
| dont_filter | 禁止使用单击的方式提交                                       |
| errback     | 处理异常要调用的函数                                         |

#### meta的特殊扩展

​	dont_redirect:遇到304是否重定向

​	dont_retry:是否不要执行错误重试

​	handle_httpstatus_list:处理HTTP状态码列表

​	dont_merge_cookies:是否不要合并Cookies

​	cookiesjar:存储cookies

​	redirect_urls:重定向URL列表

​	bindaddress:用指定的IP作为请求地址              

### FormRequest对象

​	继承于Request对象用于处理表单元素

​	class scrapy.http.FormRequest(url,[,formdata,....]):构造函数

​	formdata接收一个字典

​		FormRequest提供了一个非常有用的类方法from_response,可以通过返回的response来对表单进行预填充并返回FormRequest不过这个方法会将"提交"转换为"单击",对于JavaScript渲染网页可能无法"单击",可以将dont_filter设置为true来禁止单击

```
def from_response(cls, response, formname=None, formid=None, formnumber=0, formdata=None,
                  clickdata=None, dont_click=False, formxpath=None, formcss=None, **kwargs):
```



### 使用curl来爬取

```
#爬
curl <url>爬取网址

#参1(爬取到指定文件夹)
curl -o [文件名] <url>爬网

#参2 重定向
curl -L <url>

#参3 显示头信息(大写I只显示头信息,不显示其他信息,大写好像都是这个规律)
curl -i <url>

#参4 输出http通信过程
curl -v <url>

#参5 输出更详细
curl --trace output.txt www.baidu.com

#参6-X后加请求方式 post传递表达用--data+表单的形式
curl -X POST --data "data=xxx" <url>

#参7上传文件
假设表单为
<from method="post" enctype='multiparty/form-data' action="upload.cgi">
	<input type=file name=upload>
	<input type=submit name=press value="ok">
</form>
curl -form upload=@localfilename --form press=OK [url]

#参8表示从何处跳转来的
curl --referer <url1> <url2>

#参9添加userAgent
curl --user-agent "[userAgent]" [url]

#参10
curl --cookie "name=xxx" [url]
	扩展:
	curl -c 本地文件 [url] 将cookie保存到本地文件
	curl -d 本地文件 [url] 使用本地文件作为cookie信息,进行后续请求
#参11添加请求头
curl --header [url]
#参12登录认证
curl --user name:password [url]

```



## selenium



下载selenium用pip

chrome驱动http://chromedriver.storage.googleapis.com/index.html

```python
from selenium import webdriver
from selenium.webdriver.common.keys import Keys

if __name__ == "__main__":
    ##########################################
    #设置无头浏览器
    option = webdriver.ChromeOptions()
    option.add_argument('--headless')
    broswer = webdriver.Chrome(options=option)
    ##########################################
    # broswer = webdriver.Chrome()
    print(1)
    #请求网页
    broswer.get("http://www.baidu.com")#当网页的load加载完毕才执行用waits来确定网页是否完全加载完毕
    print(2)
    print(broswer.get_cookies())
	#关闭一个标签
    broswer.close()
    #关闭浏览器
    broswer.quit()
    
```

1.拖动

​	element = driver.find_element_by_name("source")

​	target = driver.find_element_by_name("target")

​	from selenium.webdriver import ActionChains

​	action_chains = ActionChains(element)

​	action_chains.drag_and_drop(element,target).perform()

2.切换窗口

​	driver.switch_to_window("windowName")

​	windowName可以从网页中获取,或者用driver.window_handle来迭代打开的窗口

​	**driver.switch_to_window更多的用法上网查吧**

3.定位

​	一般用XPath

4.等待事件

- 显示

  - ```python
    #默认ExpectedConditions每500毫秒一次对网页检查,如果找到了id为about的元素就停止检查交出控制权,如果超出了10秒还没有检查到就强行结束
    #更多ExpectedConditions用法看https://blog.csdn.net/weixin_42575593/article/details/83304005
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as ES
    broswer = webdriver.Chrome()
    print(1)
    broswer.get("http://www.baidu.com")
    try:
        element = WebDriverWait(broswer,10).until(ES.presence_of_all_elements_located((By.ID,"about")))
    finally:
        pass
    ```

- 隐式

  - ```python
    #虽然不推荐这么设置但是真好用
    drive.implicitly_wait(10)
    k = drive.find_element_by_xpath("//div[@id='content_left']//a")
    ```

5.行为链

​	拖动就属于行为链、完成一些键盘鼠标的交互效果。

​	详情请看网址https://www.kancloud.cn/sallymuyi/selenium/1384612

6.Cookies

```python
drive.get_cookies()
drive.get_cookie()
drive.add_cookie()
drive.delete_all_cookies()
drive.delete_cookie()
```

用selenium来接入scrapy只需要在下载中间件中process_request函数设立request请求方式并返回一个response对象如:

```python
        driver = webdriver.Chrome()
        driver.get(request.url)
        html = driver.page_source
        return HtmlResponse(url=request.url,body=html.encode(),request=request)
```

## 关于splash的一些问题

有可能网上教安装完后默认开启端口是localhost:8050端口但是我们需要在docker虚拟机上用

> docker-machine ip default

进行查看docker运行的IP查看

> http://192.168.99.100:8050/render.json?url=http://www.taobao.com

测试判断是否启动成功(url后面必须加http://)

## 其他

> 报错
>
> ​	EOL while scanning string literal
>
> 解决乱码问题
>
> ```
> #setting文件中添加这行代码
> FEED_EXPORT_ENCODING = 'utf-8'
> ```
>
> 修改PIP源地址
>
> ​	**pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pandas**
>
> ```
> text = response.xpath("//form[@id = 'frmLogin']//input[@type='hidden']/@value")
> for i in text:
>     print(i.extract())
> 获取xpath提取的内容
> ```

# 处理Excel和CSV以后再看吧现在用不到

# scrapy心得（项目心得）

1.设置cookies

​	我的做法是通过middlewares来提前获取一个cookies然后重复使用

```python
def process_request(self, request, spider):
    request.cookies = self.cookies
    if (request.cookies == {} and request.url!="http://210.44.64.139/robots.txt"):
        print("--------------------cookies-----------------------")
        print(request.cookies)
        
        session = requests.session()
        response = session.get(request.url)
        #将requests的cookies转换为dict类
        self.cookies = request.cookies = response.cookies.get_dict()
        images = session.get("http://210.44.64.139/seat/captcha.php")
        #写入二维码
        with open("./ver.png","wb") as file:
            file.write(images.content)
        a = input("请输入验证码")
        data = {
            "postdata[username]":request.meta['name'],
            "postdata[password]":request.meta['password'],
            "postdata[captcha]": a
        }
        req_header = {
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36',
        }
        #进行登录
        response = session.post(url="http://210.44.64.139/seat/seatOrderAction.php?action=normalLogin",data=data,cookies = response.cookies,headers=req_header)
        response.encoding = "utf-8"
        #返回登录结果
        print(response.text)
    return None
```

2.ROBOTSTXT_OBEY=False用来屏蔽robots 协议，这样就不会爬取robots

3.from urllib.request import urlretrieve用这个类来下载图片，超好用

4.通过py代码来开启scrapy

```python
from scrapy import cmdline
cmdline.execute(["scrapy", "crawl","-a","username=18026240010","-a","password=000000","wfu"])
```

5.与mysql数据库链接

```python
import pymysql
from .settings import *
class GetsetPipeline:
def __init__(self):
    self.connect = pymysql.connect(host=MYSQL_HOST,
                                   user=MYSQL_USER,
                                   password=MYSQL_PASSWD,
                                   database=MYSQL_DBNAME,
                                   charset='utf8',
                                   use_unicode=True,
                                   port=3306)
    self.cursor = self.connect.cursor()
def process_item(self, item, spider):
    print(self.cursor.execute("select * from seating;"))
    print("---------------------------------Pipeline-----------------------------------------")
    self.cursor.execute("""
        insert into seating(area,seat,seatId) value (%s,%s,%s );
    """,(item['area'],item['set'],item['setId']))
    self.connect.commit()
    return item
```

6.spider主体代码

```python
    #用来获取命令行的输入
    def __init__(self,username=None,password = None,*args, **kwargs):
        self.username = username
        self.password = password
        print(username+":"+password)
    #建立与mysql的链接
    def connect(self):
        self.con = pymysql.connect(host=MYSQL_HOST,
                                       user=MYSQL_USER,
                                       password=MYSQL_PASSWD,
                                       database=MYSQL_DBNAME,
                                       charset='utf8',
                                       use_unicode=True,
                                       port=3306)
        self.cursor = self.con.cursor()
	#获取MySQL数据库里面可以爬取的数据与显示要爬取区域的信息
    def Options(self):
        self.connect()
        sql = "select distinct area from seating;"
        self.cursor.execute(sql)
        result = self.cursor.fetchall()
        No = 1
        for i in result:
            print("%d-%s"%(No,i[0]))
            No+=1
        while True:
            occupy = int(input("please enter the floor you want to occupy:"))
            if occupy>0 and occupy<len(result):
                result = result[occupy-1][0]
                return result
	#设计倒计时
    def timer(self):
        now = datetime.datetime.now()
        t_now = now + timedelta(days=1)
        t_now = datetime.datetime(t_now.year, t_now.month, t_now.day, 0, 0)
        count = t_now - now
        tt = count.seconds
        while tt > 0:
            now = datetime.datetime.now()
            print(f"\r在 {count} 秒后开始", end='')
            #####################
            time.sleep(1)
            #####################
            count = t_now - now
            tt = count.seconds
    #发送一个请求用来获取cookies
    def start_requests(self):

        login_info = {"name":self.username,"password":self.password}
        yield Request(url="http://210.44.64.139/seat",callback=self.parse,meta=login_info)
        
	#正式的抢座程序
    def parse(self, response):
        # print(response.text)
        result = self.Options()
        sql = "select * from seating where area = '%s';" % (result)
        self.cursor.execute(sql)
        result = self.cursor.fetchall()
        list = random.sample(result, k=3)
        now = datetime.datetime.now()
        t_now = now + timedelta(days=1)
        time.sleep(120)
        for jj in list:
            #循环发送三个抢座请求
            fordata = {"seat_id": jj[2],
                       "order_date": "{0:04d}-{1:02d}-{2:02d}".format(t_now.year, t_now.month, t_now.day)}
            yield FormRequest(url="http://210.44.64.139/seat/seatOrderAction.php?action=addOrderSeat",
                              callback=self.get_result,
                              formdata=fordata)

    def get_result(self,response):
        print(response.text)
#关闭连接
def close(spider, reason):
        spider.cursor.close()
        spider.con.close()
```

## 打包程序

安装pyinstaller

然后用命令行在要打包的程序的目录下输入pyinstaller -F <程序名例如counter.py>





# pipelines

用来存储传递来的数据

```python
# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


# useful for handling different item types with a single interface
from itemadapter import ItemAdapter
import pymysql
from .settings import *
from .spiders import school_info
from .spiders import getCodeSpecialty
class SpiderdataPipeline:


    def open_spider(self,spider):
        self.con = pymysql.connect(host=MYSQL_HOST,
                                       user=MYSQL_USER,
                                       password=MYSQL_PASSWD,
                                       database=MYSQL_DBNAME,
                                       charset='utf8',
                                       use_unicode=True,
                                       port=3306)
        self.cursor = self.con.cursor()


    def process_item(self, item, spider):
        print(item)
        if isinstance(spider,school_info.SchoolInfoSpider):

            values = (item['schoolName']
                    , item['schoolBelong']
                    , item['levelName']
                    , item['typeName']
                    , item['dualClassName']
                    , item['address']
                    , item['content']
                    , item['isSeal']
                    , item['numSubject']
                    , item['numMaster']
                    , item['numDoctor'])

            sql = "INSERT INTO `ceedb`.`school_info` (`school_name`,`school_belong`,`levelName`,`typeName`,`dualClassName`,`address`,`content`,`isSeal`,`numSubject`,`numMaster`,`numDoctor`)VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s);"
            try:
                self.cursor.execute(sql, values)
                self.con.commit()

            except:
                print("insert failed")
                print(item)

            return item

        if isinstance(spider,getCodeSpecialty.GetcodespecialtySpider):
            values = (item['name'],item['code'],item['cateCode'],item['category'],item['subject'],item['subCode'])

            sql = "INSERT INTO `ceedb`.`codespecialty`(`name`,`code`,`cateCode`,`category`,`subject`,`subCode`)VALUES(%s,%s,%s,%s,%s,%s);"
            try:
                self.cursor.execute(sql, values)
                self.con.commit()

            except:
                print("insert failed")
                print(item)

            return item

    def close_spider(self,spider):
        self.cursor.close()
        self.con.close()
```

open_spider开启爬虫时运行的代码

close_spider关闭爬虫时运行的代码

process_item用来接收上层传递来的数据如果接收到None就直接跳过



# items

```python
# Define here the models for your scraped items
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/items.html

import scrapy


class SpiderdataItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()

    pass

class CodeSpecialty(scrapy.Item):
    cateCode = scrapy.Field()
    category = scrapy.Field()
    name = scrapy.Field()
    subject = scrapy.Field()
    subCode = scrapy.Field()
    code = scrapy.Field()


class SchoolInfo(scrapy.Item):

    schoolName = scrapy.Field()
    schoolBelong = scrapy.Field()
    levelName = scrapy.Field()
    typeName = scrapy.Field()
    dualClassName = scrapy.Field()
    address = scrapy.Field()
    content = scrapy.Field()
    isSeal = scrapy.Field()
    numSubject = scrapy.Field()
    numMaster = scrapy.Field()
    numDoctor = scrapy.Field()
```

用来存储爬取下来数据整理到一个对象种,用于scrapy中传递数据



# spider

```python
import scrapy
from ..items import SchoolInfo

class SchoolInfoSpider(scrapy.Spider):
    name = 'school_info'
    allowed_domains = ['static-data.eol.cn']
    start_urls = ['https://static-data.eol.cn/www/2.0/school/30/info.json']
    sign = True


    def parse(self, response):
        number = 31
        while self.sign:
            url = "https://static-data.eol.cn/www/2.0/school/"+str(number)+"/info.json"
            number += 1
            yield scrapy.Request(url=url, callback=self.parse_url)

    def parse_url(self,response):
        SI = SchoolInfo()
        result = response.text.encode('utf-8').decode('unicode-escape')
        jsonDict = eval(result)
        if jsonDict != "":
            dataDict = jsonDict['data']
            ################################
            schoolName = dataDict['name']
            schoolBelong = dataDict['belong']
            levelName = dataDict['level_name']
            typeName = dataDict['type_name']
            dualClassName = dataDict['dual_class_name']
            address = dataDict['address']
            content = dataDict['content']
            isSeal = dataDict['is_seal']
            numSubject = dataDict['num_subject']
            numMaster = dataDict['num_master']
            numDoctor = dataDict['num_doctor']

            SI['schoolName'] = schoolName
            SI['schoolBelong'] = schoolBelong
            SI['levelName'] = levelName
            SI['typeName'] = typeName
            SI['dualClassName'] = dualClassName
            SI['address'] = address
            SI['content'] = content
            SI['isSeal'] = isSeal
            SI['numSubject'] = numSubject
            SI['numMaster'] = numMaster
            SI['numDoctor'] = numDoctor

            print(SI)

            yield SI
        else:
            self.sign=False
            return None
```

name用来表明爬虫名称

allowed_domains允许的爬取范围

start_urls第一个爬取的地址

从start_urls爬取的数据会返回到parse中在parse中可以返回scrapy.Item类型或者一个请求、